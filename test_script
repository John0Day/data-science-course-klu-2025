#!/usr/bin/env python3
"""
Analyze flight delays for 2024 data.

The script cleans the raw CSV in chunks (to handle the full dataset size) and
summarizes which flights were delayed and what likely caused the delays. It also
breaks down delays by carrier, airport, route, month, weekday, and departure
hour, plus aggregates total delay minutes per cause.
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Iterable, Optional

import pandas as pd
from pandas import DataFrame, Series

from event_utils import load_events_map

DATA_PATH = Path("rawdata/flight_data_2024.csv")
SAMPLE_PATH = Path("rawdata/flight_data_2024_sample.csv")
EVENTS_PATH = Path("rawdata/us_events_2024.csv")

# FAA convention: arrival delay of 15 minutes or more counts as a delayed flight.
ARRIVAL_DELAY_THRESHOLD = 15
DEFAULT_CHUNK_SIZE = 250_000
DEFAULT_MIN_AIRPORT_FLIGHTS = 1000

# Columns needed for the delay analysis.
USECOLS = [
    "year",
    "month",
    "day_of_month",
    "day_of_week",
    "fl_date",
    "op_unique_carrier",
    "op_carrier_fl_num",
    "origin",
    "dest",
    "crs_dep_time",
    "dep_time",
    "dep_delay",
    "taxi_out",
    "wheels_off",
    "wheels_on",
    "taxi_in",
    "crs_arr_time",
    "arr_time",
    "arr_delay",
    "cancelled",
    "cancellation_code",
    "diverted",
    "crs_elapsed_time",
    "actual_elapsed_time",
    "air_time",
    "distance",
    "carrier_delay",
    "weather_delay",
    "nas_delay",
    "security_delay",
    "late_aircraft_delay",
]

# Memory-conscious dtype mapping based on the data dictionary.
DTYPES = {
    "year": "int16",
    "month": "int8",
    "day_of_month": "int8",
    "day_of_week": "int8",
    "op_unique_carrier": "category",
    "op_carrier_fl_num": "float32",
    "origin": "category",
    "dest": "category",
    "crs_dep_time": "Int32",
    "dep_time": "float32",
    "dep_delay": "float32",
    "taxi_out": "float32",
    "wheels_off": "float32",
    "wheels_on": "float32",
    "taxi_in": "float32",
    "crs_arr_time": "Int32",
    "arr_time": "float32",
    "arr_delay": "float32",
    "cancelled": "int8",
    "cancellation_code": "category",
    "diverted": "int8",
    "crs_elapsed_time": "float32",
    "actual_elapsed_time": "float32",
    "air_time": "float32",
    "distance": "float32",
    "carrier_delay": "float32",
    "weather_delay": "float32",
    "nas_delay": "float32",
    "security_delay": "float32",
    "late_aircraft_delay": "float32",
}

DELAY_REASON_COLS = [
    "carrier_delay",
    "weather_delay",
    "nas_delay",
    "security_delay",
    "late_aircraft_delay",
]


def read_in_chunks(
    use_sample: bool = False,
    nrows: Optional[int] = None,
    chunk_size: Optional[int] = DEFAULT_CHUNK_SIZE,
) -> Iterable[DataFrame]:
    """
    Yield pandas DataFrames in manageable chunks so we can process the full dataset.
    """
    path = SAMPLE_PATH if use_sample else DATA_PATH
    reader = pd.read_csv(
        path,
        usecols=USECOLS,
        dtype=DTYPES,
        parse_dates=["fl_date"],
        na_values=["", "NA", "NaN"],
        chunksize=chunk_size,
        nrows=nrows,
        low_memory=False,
    )
    if isinstance(reader, DataFrame):
        yield reader
    else:
        yield from reader


def clean_chunk(df: DataFrame) -> DataFrame:
    """
    Drop cancelled/diverted flights, coerce numeric columns, and tag delay reasons.
    """
    cleaned = df.copy()
    cleaned = cleaned[(cleaned["cancelled"] == 0) & (cleaned["diverted"] == 0)]

    numeric_cols = [
        "dep_delay",
        "arr_delay",
        "taxi_out",
        "wheels_off",
        "wheels_on",
        "taxi_in",
        "arr_time",
        "crs_arr_time",
        "crs_dep_time",
        "actual_elapsed_time",
        "air_time",
        "distance",
        "crs_elapsed_time",
        *DELAY_REASON_COLS,
    ]
    cleaned[numeric_cols] = cleaned[numeric_cols].apply(
        pd.to_numeric, errors="coerce"
    )
    cleaned["dep_hour"] = pd.to_numeric(
        cleaned["crs_dep_time"] // 100, errors="coerce"
    ).astype("Int16")

    cleaned = cleaned[cleaned["arr_delay"].notna()]
    cleaned["is_delayed_arr"] = cleaned["arr_delay"] >= ARRIVAL_DELAY_THRESHOLD

    reason_flags = cleaned[DELAY_REASON_COLS].gt(0)
    cleaned["reason"] = "unknown"
    for col in DELAY_REASON_COLS:
        mask = reason_flags[col] & (cleaned["reason"] == "unknown")
        cleaned.loc[mask, "reason"] = col

    return cleaned


def summarize_delays(chunks: Iterable[DataFrame], events_map: Optional[dict] = None) -> dict:
    total_rows = 0
    processed_rows = 0
    delayed_rows = 0
    arr_delay_sum_all = 0.0
    arr_delay_count_all = 0
    arr_delay_sum_delayed = 0.0
    max_arr_delay = None

    reason_counts = Series(dtype="int64")
    carrier_counts = Series(dtype="int64")
    origin_counts = Series(dtype="int64")  # delayed flights per origin
    dest_counts = Series(dtype="int64")  # delayed flights per destination
    origin_total = Series(dtype="int64")  # all flights per origin
    dest_total = Series(dtype="int64")  # all flights per destination
    origin_delay_minutes = Series(dtype="float64")
    dest_delay_minutes = Series(dtype="float64")
    route_counts = Series(dtype="int64")
    month_total = Series(dtype="int64")
    month_delayed = Series(dtype="int64")
    weekday_total = Series(dtype="int64")
    weekday_delayed = Series(dtype="int64")
    hour_total = Series(dtype="int64")
    hour_delayed = Series(dtype="int64")
    reason_delay_minutes = Series(dtype="float64")
    event_total = Series(dtype="int64")
    event_delayed = Series(dtype="int64")
    event_delay_minutes = Series(dtype="float64")
    event_origin_total = Series(dtype="int64")  # (event, origin) flights
    event_origin_delayed = Series(dtype="int64")  # (event, origin) delayed flights
    event_origin_delay_minutes = Series(dtype="float64")
    sample_delays = []

    for chunk in chunks:
        total_rows += len(chunk)
        cleaned = clean_chunk(chunk)
        processed_rows += len(cleaned)

        if cleaned.empty:
            continue

        arr_delay_sum_all += cleaned["arr_delay"].sum(skipna=True)
        arr_delay_count_all += cleaned["arr_delay"].notna().sum()

        delayed_mask = cleaned["is_delayed_arr"]
        delayed_rows += delayed_mask.sum()
        arr_delay_sum_delayed += cleaned.loc[delayed_mask, "arr_delay"].sum(
            skipna=True
        )
        if not cleaned["arr_delay"].empty:
            chunk_max = cleaned["arr_delay"].max()
            if max_arr_delay is None or chunk_max > max_arr_delay:
                max_arr_delay = chunk_max

        reason_counts = reason_counts.add(
            cleaned.loc[delayed_mask, "reason"].value_counts(), fill_value=0
        )
        carrier_counts = carrier_counts.add(
            cleaned.loc[delayed_mask]
            .groupby("op_unique_carrier", observed=True)
            .size(),
            fill_value=0,
        )
        origin_counts = origin_counts.add(
            cleaned.loc[delayed_mask].groupby("origin", observed=True).size(),
            fill_value=0,
        )
        dest_counts = dest_counts.add(
            cleaned.loc[delayed_mask].groupby("dest", observed=True).size(),
            fill_value=0,
        )
        origin_total = origin_total.add(
            cleaned.groupby("origin", observed=True).size(), fill_value=0
        )
        dest_total = dest_total.add(
            cleaned.groupby("dest", observed=True).size(), fill_value=0
        )
        origin_delay_minutes = origin_delay_minutes.add(
            cleaned.loc[delayed_mask]
            .groupby("origin", observed=True)["arr_delay"]
            .sum(),
            fill_value=0,
        )
        dest_delay_minutes = dest_delay_minutes.add(
            cleaned.loc[delayed_mask]
            .groupby("dest", observed=True)["arr_delay"]
            .sum(),
            fill_value=0,
        )
        route_counts = route_counts.add(
            cleaned.loc[delayed_mask, "origin"]
            .astype("string")
            .str.cat(cleaned.loc[delayed_mask, "dest"].astype("string"), sep="-")
            .value_counts(),
            fill_value=0,
        )
        month_total = month_total.add(cleaned["month"].value_counts(), fill_value=0)
        month_delayed = month_delayed.add(
            cleaned.loc[delayed_mask, "month"].value_counts(), fill_value=0
        )
        weekday_total = weekday_total.add(
            cleaned["day_of_week"].value_counts(), fill_value=0
        )
        weekday_delayed = weekday_delayed.add(
            cleaned.loc[delayed_mask, "day_of_week"].value_counts(), fill_value=0
        )
        hour_total = hour_total.add(
            cleaned["dep_hour"].value_counts(), fill_value=0
        )
        hour_delayed = hour_delayed.add(
            cleaned.loc[delayed_mask, "dep_hour"].value_counts(), fill_value=0
        )
        reason_delay_minutes = reason_delay_minutes.add(
            cleaned.loc[delayed_mask].groupby("reason", observed=True)["arr_delay"].sum(),
            fill_value=0,
        )
        if events_map:
            cleaned["event_name"] = cleaned["fl_date"].dt.date.map(events_map).fillna("none")
            event_total = event_total.add(
                cleaned["event_name"].value_counts(), fill_value=0
            )
            event_delayed = event_delayed.add(
                cleaned.loc[delayed_mask, "event_name"].value_counts(), fill_value=0
            )
            event_delay_minutes = event_delay_minutes.add(
                cleaned.loc[delayed_mask]
                .groupby("event_name", observed=True)["arr_delay"]
                .sum(),
                fill_value=0,
            )
            # per-origin breakdown on event days
            event_origin_total = event_origin_total.add(
                cleaned.groupby(["event_name", "origin"], observed=True).size(),
                fill_value=0,
            )
            event_origin_delayed = event_origin_delayed.add(
                cleaned.loc[delayed_mask]
                .groupby(["event_name", "origin"], observed=True)
                .size(),
                fill_value=0,
            )
            event_origin_delay_minutes = event_origin_delay_minutes.add(
                cleaned.loc[delayed_mask]
                .groupby(["event_name", "origin"], observed=True)["arr_delay"]
                .sum(),
                fill_value=0,
            )

        if len(sample_delays) < 10:
            sample_rows = cleaned.loc[
                delayed_mask,
                [
                    "fl_date",
                    "op_unique_carrier",
                    "op_carrier_fl_num",
                    "origin",
                    "dest",
                    "arr_delay",
                    "reason",
                ],
            ].head(10 - len(sample_delays))
            sample_delays.extend(sample_rows.to_dict(orient="records"))

    return {
        "total_rows": total_rows,
        "processed_rows": processed_rows,
        "delayed_rows": delayed_rows,
        "delay_rate_pct": (delayed_rows / processed_rows * 100) if processed_rows else 0,
        "arr_delay_mean_all": (arr_delay_sum_all / arr_delay_count_all)
        if arr_delay_count_all
        else 0,
        "arr_delay_mean_delayed": (arr_delay_sum_delayed / delayed_rows)
        if delayed_rows
        else 0,
        "max_arr_delay": max_arr_delay,
        "reason_counts": reason_counts.sort_values(ascending=False).astype(int),
        "reason_delay_minutes": reason_delay_minutes.sort_values(
            ascending=False
        ),
        "carrier_counts": carrier_counts.sort_values(ascending=False).astype(int),
        "origin_counts": origin_counts.sort_values(ascending=False).astype(int),
        "dest_counts": dest_counts.sort_values(ascending=False).astype(int),
        "origin_total": origin_total.sort_values(ascending=False).astype(int),
        "dest_total": dest_total.sort_values(ascending=False).astype(int),
        "origin_delay_minutes": origin_delay_minutes.sort_values(
            ascending=False
        ),
        "dest_delay_minutes": dest_delay_minutes.sort_values(ascending=False),
        "route_counts": route_counts.sort_values(ascending=False).astype(int),
        "month_rates": (month_delayed / month_total * 100).sort_index(),
        "weekday_rates": (weekday_delayed / weekday_total * 100).sort_index(),
        "hour_rates": (hour_delayed / hour_total * 100).sort_index(),
        "event_total": event_total.sort_values(ascending=False).astype(int)
        if not event_total.empty
        else Series(dtype="int64"),
        "event_delayed": event_delayed.sort_values(ascending=False).astype(int)
        if not event_delayed.empty
        else Series(dtype="int64"),
        "event_delay_minutes": event_delay_minutes.sort_values(
            ascending=False
        )
        if not event_delay_minutes.empty
        else Series(dtype="float64"),
        "event_origin_total": event_origin_total.sort_values(ascending=False).astype(int)
        if not event_origin_total.empty
        else Series(dtype="int64"),
        "event_origin_delayed": event_origin_delayed.sort_values(ascending=False).astype(int)
        if not event_origin_delayed.empty
        else Series(dtype="int64"),
        "event_origin_delay_minutes": event_origin_delay_minutes.sort_values(
            ascending=False
        )
        if not event_origin_delay_minutes.empty
        else Series(dtype="float64"),
        "sample_delays": sample_delays,
    }


def print_summary(summary: dict, top_n: int = 10, min_airport_flights: int = DEFAULT_MIN_AIRPORT_FLIGHTS) -> None:
    print(f"Rows in file: {summary['total_rows']:,}")
    print(f"Rows after cleaning (no cancellations/diversions): {summary['processed_rows']:,}")
    print(
        f"Delayed flights (arr_delay >= {ARRIVAL_DELAY_THRESHOLD}): {summary['delayed_rows']:,} "
        f"({summary['delay_rate_pct']:.2f}% of clean flights)"
    )
    print(
        f"Mean arr_delay (all flights with data): {summary['arr_delay_mean_all']:.2f} minutes; "
        f"mean among delayed flights: {summary['arr_delay_mean_delayed']:.2f} minutes; "
        f"max observed arrival delay: {summary['max_arr_delay']}"
    )

    print("\nDelay reasons (flights with positive minutes in the column):")
    if summary["reason_counts"].empty:
        print("  No delays detected.")
    else:
        print(summary["reason_counts"].to_string())
        print("\nTotal arrival delay minutes attributed to each reason:")
        print(summary["reason_delay_minutes"].to_string())

    print(f"\nTop {top_n} carriers by delayed flights:")
    print(summary["carrier_counts"].head(top_n).to_string())

    print(f"\nTop {top_n} origins by delayed flights:")
    print(summary["origin_counts"].head(top_n).to_string())

    print(f"\nTop {top_n} destinations by delayed flights:")
    print(summary["dest_counts"].head(top_n).to_string())

    # Airport delay stats (rate and total delay minutes) with a minimum flight threshold
    origin_stats = pd.DataFrame(
        {
            "flights": summary["origin_total"],
            "delayed": summary["origin_counts"],
            "delay_minutes": summary["origin_delay_minutes"],
        }
    )
    origin_stats["delay_rate_pct"] = (
        origin_stats["delayed"] / origin_stats["flights"] * 100
    )
    origin_stats = origin_stats[
        origin_stats["flights"] >= min_airport_flights
    ].sort_values("delay_rate_pct", ascending=False)

    dest_stats = pd.DataFrame(
        {
            "flights": summary["dest_total"],
            "delayed": summary["dest_counts"],
            "delay_minutes": summary["dest_delay_minutes"],
        }
    )
    dest_stats["delay_rate_pct"] = (
        dest_stats["delayed"] / dest_stats["flights"] * 100
    )
    dest_stats = dest_stats[
        dest_stats["flights"] >= min_airport_flights
    ].sort_values("delay_rate_pct", ascending=False)

    print(
        f"\nTop {top_n} origin airports by delay rate (min {min_airport_flights} flights):"
    )
    print(origin_stats.head(top_n)[["flights", "delayed", "delay_rate_pct", "delay_minutes"]].to_string())

    print(
        f"\nTop {top_n} destination airports by delay rate (min {min_airport_flights} flights):"
    )
    print(dest_stats.head(top_n)[["flights", "delayed", "delay_rate_pct", "delay_minutes"]].to_string())

    print(f"\nTop {top_n} routes by delayed flights:")
    print(summary["route_counts"].head(top_n).to_string())

    print("\nDelay rate by month (%):")
    print(summary["month_rates"].to_string())

    print("\nDelay rate by day_of_week (%): 1=Mon ... 7=Sun (per DOT convention).")
    print(summary["weekday_rates"].to_string())

    print("\nDelay rate by scheduled departure hour (%):")
    print(summary["hour_rates"].to_string())

    if not summary["event_total"].empty:
        event_rates = (summary["event_delayed"] / summary["event_total"] * 100).sort_values(
            ascending=False
        )
        print("\nDelay rate by notable event day (%), including 'none' for non-event days:")
        event_df = pd.DataFrame(
            {
                "flights": summary["event_total"],
                "delayed": summary["event_delayed"],
                "delay_rate_pct": event_rates,
                "delay_minutes": summary["event_delay_minutes"],
            }
        ).fillna(0)
        print(event_df.head(top_n if "none" not in event_df.index else top_n + 1).to_string())
        # per-airport impact: compare each event day vs that airport's non-event baseline
        if not summary["event_origin_total"].empty:
            event_origin_df = pd.DataFrame(
                {
                    "flights": summary["event_origin_total"],
                    "delayed": summary["event_origin_delayed"],
                    "delay_minutes": summary["event_origin_delay_minutes"],
                }
            ).fillna(0)
            event_origin_df["delay_rate_pct"] = (
                event_origin_df["delayed"] / event_origin_df["flights"] * 100
            )
            if ("none" in summary["event_total"].index) and (
                "none" in event_origin_df.index.get_level_values(0)
            ):
                baseline_by_origin = (
                    event_origin_df.xs("none", level=0)["delay_rate_pct"]
                )
                event_origin_df["baseline_rate_origin"] = (
                    event_origin_df.index.get_level_values(1).map(baseline_by_origin)
                )
                event_origin_df["rate_delta_vs_origin_baseline"] = (
                    event_origin_df["delay_rate_pct"]
                    - event_origin_df["baseline_rate_origin"]
                )
            else:
                event_origin_df["baseline_rate_origin"] = pd.NA
                event_origin_df["rate_delta_vs_origin_baseline"] = pd.NA

            event_origin_df = event_origin_df.reset_index(names=["event", "origin"])
            event_origin_df = event_origin_df[
                (event_origin_df["event"] != "none")
                & (event_origin_df["flights"] >= min_airport_flights)
            ]
            event_origin_df = event_origin_df.sort_values(
                ["rate_delta_vs_origin_baseline", "delay_rate_pct"],
                ascending=False,
                na_position="last",
            )
            print(
                f"\nTop {top_n} event+origin pairs by rate delta vs origin non-event baseline "
                f"(min {min_airport_flights} flights on that event day):"
            )
            cols = [
                "event",
                "origin",
                "flights",
                "delayed",
                "delay_rate_pct",
                "baseline_rate_origin",
                "rate_delta_vs_origin_baseline",
                "delay_minutes",
            ]
            print(event_origin_df[cols].head(top_n).to_string(index=False))

    if summary["sample_delays"]:
        print("\nSample delayed flights:")
        for row in summary["sample_delays"]:
            print(
                f"{row['fl_date'].date()} | {row['op_unique_carrier']} {int(row['op_carrier_fl_num'])} "
                f"{row['origin']} -> {row['dest']} | arr_delay={row['arr_delay']} | reason={row['reason']}"
            )


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Clean flight data and summarize delay reasons."
    )
    parser.add_argument(
        "--sample",
        action="store_true",
        help="Use the smaller sample file for a quick run.",
    )
    parser.add_argument(
        "--nrows",
        type=int,
        default=None,
        help="Only read the first N rows (for smoke testing).",
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=DEFAULT_CHUNK_SIZE,
        help="Chunk size for streaming the CSV. Lower this if memory is tight.",
    )
    parser.add_argument(
        "--top",
        type=int,
        default=10,
        help="How many carriers/origins/destinations to show in the summary.",
    )
    parser.add_argument(
        "--min-airport-flights",
        type=int,
        default=DEFAULT_MIN_AIRPORT_FLIGHTS,
        help="Minimum flights for an airport to appear in rate summaries.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    events_map = load_events_map(EVENTS_PATH)
    chunks = read_in_chunks(
        use_sample=args.sample,
        nrows=args.nrows,
        chunk_size=args.chunk_size,
    )
    summary = summarize_delays(chunks, events_map=events_map)
    print_summary(summary, top_n=args.top, min_airport_flights=args.min_airport_flights)


if __name__ == "__main__":
    main()
